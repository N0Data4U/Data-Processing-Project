{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "- Standardizing Data?\n",
    "- taking natural log of phone prices as dependent variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import shap\n",
    "\n",
    "# a classic housing price dataset\n",
    "X, y = shap.datasets.california(n_points=1000)\n",
    "\n",
    "X100 = shap.utils.sample(X, 100)  # 100 instances for use as the background distribution\n",
    "\n",
    "# a simple linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model coefficients:\\n\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model.coef_[i].round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    ice=False,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of SHAP Value, as difference between partial dependence plot (at the feature's values) and expected model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "explainer = shap.Explainer(model.predict, X100)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# make a standard partial dependence plot\n",
    "sample_ind = 20\n",
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    ice=False,\n",
    "    shap_values=shap_values[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The close correspondence between the classic partial dependence plot and SHAP values means that if we plot the SHAP value for a specific feature across a whole dataset we will exactly trace out a mean centered version of the partial dependence plot for that feature\". Mean-centered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive Nature of SHAP values: \"For machine learning models this means that SHAP values of all the input features will always sum up to the difference between baseline (expected) model output and the current model output for the prediction being explained\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The reason the partial dependence plots of linear models have such a close connection to SHAP values is because each feature in the model is handled independently of every other feature (the effects are just added together). We can keep this additive nature while relaxing the linear requirement of straight lines.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generalized Additive Regression Model (GAM's) - InterpretMLs explainable boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a GAM model to the data\n",
    "import interpret.glassbox\n",
    "\n",
    "model_ebm = interpret.glassbox.ExplainableBoostingRegressor(interactions=0)\n",
    "model_ebm.fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_ebm = shap.Explainer(model_ebm.predict, X100)\n",
    "shap_values_ebm = explainer_ebm(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_ebm.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_ebm[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-centered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_ebm[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values_ebm[sample_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.beeswarm(shap_values_ebm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Non-additive boosted tree-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "import xgboost\n",
    "\n",
    "model_xgb = xgboost.XGBRegressor(n_estimators=100, max_depth=2).fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_xgb.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_xgb[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-centered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Linear logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a classic adult census dataset price dataset\n",
    "X_adult, y_adult = shap.datasets.adult()\n",
    "\n",
    "# a simple linear logistic model\n",
    "model_adult = sklearn.linear_model.LogisticRegression(max_iter=10000)\n",
    "model_adult.fit(X_adult, y_adult)\n",
    "\n",
    "\n",
    "def model_adult_proba(x):\n",
    "    return model_adult.predict_proba(x)[:, 1]\n",
    "\n",
    "\n",
    "def model_adult_log_odds(x):\n",
    "    p = model_adult.predict_log_proba(x)\n",
    "    return p[:, 1] - p[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a standard partial dependence plot\n",
    "sample_ind = 18\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"Capital Gain\",\n",
    "    model_adult_proba,\n",
    "    X_adult,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use SHAP to explain the probability of a linear logistic regression model we see strong interaction effects. This is because a linear logistic regression model is NOT additive in the probability space. If we instead explain the log-odds output of the model we see a perfect linear relationship between the models inputs and the modelâ€™s outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "background_adult = shap.maskers.Independent(X_adult, max_samples=100)\n",
    "explainer = shap.Explainer(model_adult_proba, background_adult)\n",
    "shap_values_adult = explainer(X_adult[:1000])\n",
    "Permutation explainer: 1001it [00:58, 14.39it/s]\n",
    "shap.plots.scatter(shap_values_adult[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "explainer_log_odds = shap.Explainer(model_adult_log_odds, background_adult)\n",
    "shap_values_adult_log_odds = explainer_log_odds(X_adult[:1000])\n",
    "Permutation explainer: 1001it [01:01, 13.61it/s]\n",
    "shap.plots.scatter(shap_values_adult_log_odds[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a standard partial dependence plot\n",
    "sample_ind = 18\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"Age\",\n",
    "    model_adult_log_odds,\n",
    "    X_adult,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Non-additive boosted tree-regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "model = xgboost.XGBClassifier(n_estimators=100, max_depth=2).fit(\n",
    "    X_adult, y_adult * 1, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# compute SHAP values\n",
    "explainer = shap.Explainer(model, background_adult)\n",
    "shap_values = explainer(X_adult)\n",
    "\n",
    "# set a display version of the data to use for plotting (has string values)\n",
    "shap_values.display_data = shap.datasets.adult(display=True)[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a SHAP bar plot will take the mean absolute value of each feature over all the instances (rows) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are willing to deal with a bit more complexity, we can use a beeswarm plot to summarize the entire distribution of SHAP values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.heatmap(shap_values[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"Capital Gain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Relationship\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Dealing with correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = shap.utils.hclust(X_adult, y_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering, clustering_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering, clustering_cutoff=1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Explaining a transformers NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# load a BERT sentiment analysis model\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ").cuda()\n",
    "\n",
    "\n",
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=500, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).cuda()\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "\n",
    "# build an explainer using a token masker\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# explain the model's predictions on IMDB reviews\n",
    "imdb_train = datasets.load_dataset(\"imdb\")[\"train\"]\n",
    "shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values.abs.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values.abs.sum(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
