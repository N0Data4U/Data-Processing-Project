{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Summary Statistics and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting summary statistics for metric and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_summary_statistics(data):\n",
    "    # Separate metric and categorical variables\n",
    "    metric_variables = data.select_dtypes(include=['float', 'int'])\n",
    "    categorical_variables = data.select_dtypes(include=['object'])\n",
    "\n",
    "    # Summary statistics for metric variables\n",
    "    metric_summary = metric_variables.describe().transpose()\n",
    "\n",
    "    # Counts and percentages for categorical variables\n",
    "    categorical_summary = pd.DataFrame()\n",
    "    for col in categorical_variables.columns:\n",
    "        counts = categorical_variables[col].value_counts()\n",
    "        percentages = counts / counts.sum()\n",
    "        summary = pd.DataFrame({'Counts': counts, 'Percentages': percentages})\n",
    "        categorical_summary[col] = summary\n",
    "\n",
    "    # Correlation matrix for metric variables\n",
    "    correlation_matrix = metric_variables.corr()\n",
    "\n",
    "    return metric_summary, categorical_summary, correlation_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'your_data.csv' is your dataset file\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "metric_summary, categorical_summary, correlation_matrix = generate_summary_statistics(data)\n",
    "\n",
    "# Display the results\n",
    "print(\"Summary Statistics for Metric Variables:\")\n",
    "print(metric_summary)\n",
    "\n",
    "print(\"\\nCounts and Percentages for Categorical Variables:\")\n",
    "print(categorical_summary)\n",
    "\n",
    "print(\"\\nCorrelation Matrix for Metric Variables:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one chart with statistics comparing the three retailers. Getting average price of products, number of products. Brand with number of products that is sold the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some visualizations to analyze some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your data (assuming it's in a list of dictionaries format)\n",
    "data = [\n",
    "    {'ID': 1, 'online-retailer': 'DatArt', 'category': 'Electronics'},\n",
    "    {'ID': 2, 'online-retailer': 'GadgetZone', 'category': 'Clothing'},\n",
    "    {'ID': 3, 'online-retailer': 'TechHub', 'category': 'Electronics'},\n",
    "    {'ID': 4, 'online-retailer': 'FashionMall', 'category': 'Clothing'},\n",
    "    # Add more data as needed\n",
    "]\n",
    "\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot bar chart with percentages -> brands, online-retailers, colours, number of cores, 5G, NFC, colour\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts = df['category'].value_counts()\n",
    "category_percentages = (category_counts / category_counts.sum()) * 100\n",
    "category_percentages.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Categories')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Plot pie chart -> brands retailers, colours, number of cores, 5G, NFC, colour\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140, colors=['lightcoral', 'lightgreen'])\n",
    "plt.title('Distribution of Categories (Pie Chart)')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot -> front cam resolution, rear cam resolution, internal memory\n",
    "# Variable comparison -> prices vs. brands (highlight retailer), prices vs. internal memory (higlight brand), brand vs. internal memory, prices vs. length (should not have positive pattern), prices vs. wireless charging\n",
    "# Create subsets based on the categories\n",
    "electronics_data = df[df['category'] == 'Electronics']\n",
    "clothing_data = df[df['category'] == 'Clothing']\n",
    "# Plot scatter plot with different colors for each category\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(electronics_data['price'], electronics_data['rating'], c='blue', label='Electronics', alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "plt.scatter(clothing_data['price'], clothing_data['rating'], c='red', label='Clothing', alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "plt.title('Scatter Plot with Highlighted Categories')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Rating')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "# Convert data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Heatmap of Correlations between Metric Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "- Standardizing Data?\n",
    "- taking natural log of phone prices as dependent variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import shap\n",
    "\n",
    "# a classic housing price dataset\n",
    "X, y = shap.datasets.california(n_points=1000)\n",
    "\n",
    "X100 = shap.utils.sample(X, 100)  # 100 instances for use as the background distribution\n",
    "\n",
    "# a simple linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model coefficients:\\n\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model.coef_[i].round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    ice=False,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of SHAP Value, as difference between partial dependence plot (at the feature's values) and expected model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "explainer = shap.Explainer(model.predict, X100)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# make a standard partial dependence plot\n",
    "sample_ind = 20\n",
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    ice=False,\n",
    "    shap_values=shap_values[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The close correspondence between the classic partial dependence plot and SHAP values means that if we plot the SHAP value for a specific feature across a whole dataset we will exactly trace out a mean centered version of the partial dependence plot for that feature\". Mean-centered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive Nature of SHAP values: \"For machine learning models this means that SHAP values of all the input features will always sum up to the difference between baseline (expected) model output and the current model output for the prediction being explained\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The reason the partial dependence plots of linear models have such a close connection to SHAP values is because each feature in the model is handled independently of every other feature (the effects are just added together). We can keep this additive nature while relaxing the linear requirement of straight lines.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generalized Additive Regression Model (GAM's) - InterpretMLs explainable boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a GAM model to the data\n",
    "import interpret.glassbox\n",
    "\n",
    "model_ebm = interpret.glassbox.ExplainableBoostingRegressor(interactions=0)\n",
    "model_ebm.fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_ebm = shap.Explainer(model_ebm.predict, X100)\n",
    "shap_values_ebm = explainer_ebm(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_ebm.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_ebm[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-centered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_ebm[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values_ebm[sample_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.beeswarm(shap_values_ebm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Non-additive boosted tree-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "import xgboost\n",
    "\n",
    "model_xgb = xgboost.XGBRegressor(n_estimators=100, max_depth=2).fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_xgb.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_xgb[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-centered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Linear logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a classic adult census dataset price dataset\n",
    "X_adult, y_adult = shap.datasets.adult()\n",
    "\n",
    "# a simple linear logistic model\n",
    "model_adult = sklearn.linear_model.LogisticRegression(max_iter=10000)\n",
    "model_adult.fit(X_adult, y_adult)\n",
    "\n",
    "\n",
    "def model_adult_proba(x):\n",
    "    return model_adult.predict_proba(x)[:, 1]\n",
    "\n",
    "\n",
    "def model_adult_log_odds(x):\n",
    "    p = model_adult.predict_log_proba(x)\n",
    "    return p[:, 1] - p[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a standard partial dependence plot\n",
    "sample_ind = 18\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"Capital Gain\",\n",
    "    model_adult_proba,\n",
    "    X_adult,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use SHAP to explain the probability of a linear logistic regression model we see strong interaction effects. This is because a linear logistic regression model is NOT additive in the probability space. If we instead explain the log-odds output of the model we see a perfect linear relationship between the models inputs and the model’s outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "background_adult = shap.maskers.Independent(X_adult, max_samples=100)\n",
    "explainer = shap.Explainer(model_adult_proba, background_adult)\n",
    "shap_values_adult = explainer(X_adult[:1000])\n",
    "Permutation explainer: 1001it [00:58, 14.39it/s]\n",
    "shap.plots.scatter(shap_values_adult[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "explainer_log_odds = shap.Explainer(model_adult_log_odds, background_adult)\n",
    "shap_values_adult_log_odds = explainer_log_odds(X_adult[:1000])\n",
    "Permutation explainer: 1001it [01:01, 13.61it/s]\n",
    "shap.plots.scatter(shap_values_adult_log_odds[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a standard partial dependence plot\n",
    "sample_ind = 18\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"Age\",\n",
    "    model_adult_log_odds,\n",
    "    X_adult,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Non-additive boosted tree-regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "model = xgboost.XGBClassifier(n_estimators=100, max_depth=2).fit(\n",
    "    X_adult, y_adult * 1, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# compute SHAP values\n",
    "explainer = shap.Explainer(model, background_adult)\n",
    "shap_values = explainer(X_adult)\n",
    "\n",
    "# set a display version of the data to use for plotting (has string values)\n",
    "shap_values.display_data = shap.datasets.adult(display=True)[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a SHAP bar plot will take the mean absolute value of each feature over all the instances (rows) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are willing to deal with a bit more complexity, we can use a beeswarm plot to summarize the entire distribution of SHAP values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.heatmap(shap_values[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"Capital Gain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"Relationship\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Dealing with correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = shap.utils.hclust(X_adult, y_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering, clustering_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, clustering=clustering, clustering_cutoff=1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Explaining a transformers NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# load a BERT sentiment analysis model\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ").cuda()\n",
    "\n",
    "\n",
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=500, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).cuda()\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "\n",
    "# build an explainer using a token masker\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# explain the model's predictions on IMDB reviews\n",
    "imdb_train = datasets.load_dataset(\"imdb\")[\"train\"]\n",
    "shap_values = explainer(imdb_train[:10], fixed_context=1, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values.abs.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values.abs.sum(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
